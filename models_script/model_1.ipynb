{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary modules and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# popraw importy \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import keras\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#!pip install -q keras_metrics\n",
    "import keras_metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "#!pip install livelossplot\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytujemy poprzednio utworzone zbiory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../data/cleaned_data.csv', index_col=0)\n",
    "y = pd.read_csv('../data/labels.csv', header = None)\n",
    "\n",
    "X_test = pd.read_csv('../data/X_test.csv', index_col=0)\n",
    "X_train = pd.read_csv('../data/X_train.csv', index_col=0)\n",
    "X_train_std = pd.read_csv('../data/X_train_std.csv', index_col=0)\n",
    "X_test_std = pd.read_csv('../data/X_test_std.csv', index_col=0)\n",
    "y_train = pd.read_csv('../data/y_train.csv', index_col=0)\n",
    "y_test = pd.read_csv('../data/y_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zbuduję przykładową sieć, która składa się z warstwy wejściowej, jednej warstwy ukrytej i  warstwy wyjściowej.\n",
    "Kompilujemy model przy użyciu \n",
    "\n",
    "loss_function='binary_crossentropy';\n",
    "\n",
    "optimizer='Adam'.\n",
    "\n",
    "\n",
    "Naszą główną metryką będzie **precyzja** oznaczająca procent osób prawidłowie określonych jako zagrożone odejściem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pierwsze podejście do modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/3\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.4148 - precision: 0.6655 - acc: 0.8257 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 2/3\n",
      "6700/6700 [==============================] - 19s 3ms/step - loss: 0.3612 - precision: 0.7425 - acc: 0.8519 - val_loss: 3.3281 - val_precision: 0.0000e+00 - val_acc: 0.7930\n",
      "Epoch 3/3\n",
      "6700/6700 [==============================] - 18s 3ms/step - loss: 0.3494 - precision: 0.7769 - acc: 0.8585 - val_loss: 3.3248 - val_precision: 0.0000e+00 - val_acc: 0.7933\n",
      "3300/3300 [==============================] - 0s 80us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3608325544270602, 0.6681127981197152, 0.8433333333333334]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(11, activation='relu', input_shape=(10,)))\n",
    " \n",
    "model.add(Dense(121, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[keras_metrics.precision(),'accuracy'])\n",
    "                   \n",
    "model.fit(X_train_std, y_train,epochs=3, batch_size=1, verbose=1, validation_data=(X_test,y_test))\n",
    "\n",
    "model.evaluate(X_test_std,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprawdźmy skuteczność naszej predykcji raportem klasyfikacyjnym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91      2628\n",
      "           1       0.78      0.34      0.47       672\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      3300\n",
      "   macro avg       0.81      0.66      0.69      3300\n",
      "weighted avg       0.84      0.85      0.82      3300\n",
      "\n",
      "0.8522954091816367\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_test_std)\n",
    "report = classification_report(y_test,y_pred)\n",
    "report1 = classification_report(y_test,y_pred,output_dict=True)\n",
    "print(report)\n",
    "print(report1['0']['precision'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdźmy, jak prezentują się wyniki w zależności od liczby neuronów w warstwie ukrytej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.4463 - precision: 0.5332 - acc: 0.8009 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3698 - precision: 0.7334 - acc: 0.8499 - val_loss: 4.3105 - val_precision: 0.2488 - val_acc: 0.7309\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3577 - precision: 0.7666 - acc: 0.8549 - val_loss: 4.8452 - val_precision: 0.2445 - val_acc: 0.6970\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 19s 3ms/step - loss: 0.3520 - precision: 0.7695 - acc: 0.8560 - val_loss: 3.2965 - val_precision: 0.0000e+00 - val_acc: 0.7948\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 19s 3ms/step - loss: 0.3462 - precision: 0.7818 - acc: 0.8610 - val_loss: 6.8317 - val_precision: 0.1365 - val_acc: 0.5703\n",
      "Zakończono trenowanie modelu z 10 węzłami.\n",
      "3300/3300 [==============================] - 0s 32us/step\n",
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.4344 - precision: 0.6206 - acc: 0.8085 - val_loss: 6.6908 - val_precision: 0.2341 - val_acc: 0.5815\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3740 - precision: 0.7425 - acc: 0.8472 - val_loss: 4.4433 - val_precision: 0.2490 - val_acc: 0.7230\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 19s 3ms/step - loss: 0.3595 - precision: 0.7451 - acc: 0.8525 - val_loss: 4.2845 - val_precision: 0.2506 - val_acc: 0.7324\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3532 - precision: 0.7583 - acc: 0.8546 - val_loss: 4.1324 - val_precision: 0.2548 - val_acc: 0.7427\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 23s 3ms/step - loss: 0.3495 - precision: 0.7704 - acc: 0.8585 - val_loss: 4.1324 - val_precision: 0.2548 - val_acc: 0.7427\n",
      "Zakończono trenowanie modelu z 30 węzłami.\n",
      "3300/3300 [==============================] - 0s 33us/step\n",
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 24s 4ms/step - loss: 0.4370 - precision: 0.7139 - acc: 0.8175 - val_loss: 3.2188 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.3742 - precision: 0.7568 - acc: 0.8499 - val_loss: 6.0757 - val_precision: 0.2318 - val_acc: 0.6203\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3554 - precision: 0.7768 - acc: 0.8596 - val_loss: 8.0833 - val_precision: 0.2381 - val_acc: 0.4936\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3478 - precision: 0.7769 - acc: 0.8593 - val_loss: 6.8305 - val_precision: 0.2322 - val_acc: 0.5727\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3447 - precision: 0.7670 - acc: 0.8585 - val_loss: 8.4884 - val_precision: 0.2397 - val_acc: 0.4679\n",
      "Zakończono trenowanie modelu z 50 węzłami.\n",
      "3300/3300 [==============================] - 0s 30us/step\n",
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.4336 - precision: 0.6864 - acc: 0.8216 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3661 - precision: 0.7439 - acc: 0.8528 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 19s 3ms/step - loss: 0.3551 - precision: 0.7513 - acc: 0.8536 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3491 - precision: 0.7666 - acc: 0.8573 - val_loss: 7.2082 - val_precision: 0.1396 - val_acc: 0.5491\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3451 - precision: 0.7740 - acc: 0.8618 - val_loss: 3.2853 - val_precision: 0.0000e+00 - val_acc: 0.7961\n",
      "Zakończono trenowanie modelu z 70 węzłami.\n",
      "3300/3300 [==============================] - 0s 36us/step\n",
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.4144 - precision: 0.6996 - acc: 0.8276 - val_loss: 12.6387 - val_precision: 0.2041 - val_acc: 0.2070\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3653 - precision: 0.7293 - acc: 0.8516 - val_loss: 12.6284 - val_precision: 0.2043 - val_acc: 0.2079\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 23s 3ms/step - loss: 0.3584 - precision: 0.7532 - acc: 0.8548 - val_loss: 12.6380 - val_precision: 0.2044 - val_acc: 0.2073\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3520 - precision: 0.7781 - acc: 0.8584 - val_loss: 12.6380 - val_precision: 0.2044 - val_acc: 0.2073\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3468 - precision: 0.7705 - acc: 0.8603 - val_loss: 12.6669 - val_precision: 0.2040 - val_acc: 0.2055\n",
      "Zakończono trenowanie modelu z 90 węzłami.\n",
      "3300/3300 [==============================] - 0s 36us/step\n",
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.4359 - precision: 0.6347 - acc: 0.8139 - val_loss: 3.4448 - val_precision: 0.3229 - val_acc: 0.7861\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3641 - precision: 0.7225 - acc: 0.8482 - val_loss: 5.0962 - val_precision: 0.2329 - val_acc: 0.6755\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3518 - precision: 0.7562 - acc: 0.8581 - val_loss: 8.6511 - val_precision: 0.2396 - val_acc: 0.4579\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3453 - precision: 0.7627 - acc: 0.8613 - val_loss: 8.6511 - val_precision: 0.2396 - val_acc: 0.4579\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3433 - precision: 0.7684 - acc: 0.8616 - val_loss: 12.6390 - val_precision: 0.2041 - val_acc: 0.2070\n",
      "Zakończono trenowanie modelu z 110 węzłami.\n",
      "3300/3300 [==============================] - 0s 34us/step\n",
      "{50: [0.8627717391304348, 0.7528089887640449], 110: [0.8705110497237569, 0.7351485148514851], 70: [0.867103900586814, 0.7121588089330024], 10: [0.8619642254471819, 0.7804154302670623], 90: [0.8604651162790697, 0.7747747747747747], 30: [0.8621511232130701, 0.7375690607734806]}\n"
     ]
    }
   ],
   "source": [
    "nodes = [10, 30, 50, 70, 90, 110]\n",
    "wyniki = {}\n",
    "for i in nodes:\n",
    "    model = 0\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(11, activation='relu', input_shape=(10,)))\n",
    "\n",
    "    model.add(Dense(i, activation='relu'))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[keras_metrics.precision(), 'accuracy'])\n",
    "\n",
    "    model.fit(X_train_std, y_train, epochs=5, batch_size=1, verbose=1, validation_data=(X_test, y_test))\n",
    "    print(\"Zakończono trenowanie modelu z {0} węzłami.\".format(i))\n",
    "    model.evaluate(X_test_std, y_test)\n",
    "    y_pred = model.predict_classes(X_test_std)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    wyniki.update({i: [report['0']['precision'], report['1']['precision']]})\n",
    "\n",
    "print(wyniki)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wybieramy model z 90 węzłami - dał najlepsze wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/30\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.4434 - precision: 0.6061 - acc: 0.8099 - val_loss: 8.6511 - val_precision: 0.2399 - val_acc: 0.4579\n",
      "Epoch 2/30\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3893 - precision: 0.7113 - acc: 0.8382 - val_loss: 8.6462 - val_precision: 0.2402 - val_acc: 0.4582\n",
      "Epoch 3/30\n",
      "6700/6700 [==============================] - 29s 4ms/step - loss: 0.3697 - precision: 0.7417 - acc: 0.8485 - val_loss: 12.6619 - val_precision: 0.2041 - val_acc: 0.2058\n",
      "Epoch 4/30\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3664 - precision: 0.7317 - acc: 0.8455 - val_loss: 12.6959 - val_precision: 0.2036 - val_acc: 0.2036\n",
      "3300/3300 [==============================] - 0s 35us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.98      0.91      2628\n",
      "           1       0.81      0.36      0.50       672\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      3300\n",
      "   macro avg       0.84      0.67      0.71      3300\n",
      "weighted avg       0.85      0.85      0.83      3300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(10, activation='relu', input_shape=(10,)))\n",
    "model.add(Dropout(0.1, ))\n",
    "\n",
    "model.add(Dense(90, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[keras_metrics.precision(), 'accuracy'])\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "model.fit(X_train_std, y_train, epochs=30, batch_size=1, verbose=1, validation_data=(X_test, y_test),\n",
    "          callbacks=[early_stopping_monitor])\n",
    "\n",
    "model.evaluate(X_test_std, y_test)\n",
    "y_pred = model.predict_classes(X_test_std)\n",
    "report = classification_report(y_test, y_pred)\n",
    "report1 = classification_report(y_test, y_pred, output_dict=True)\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zapisujemy wytrenowany model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autor: Tomasz Sołtysiak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
