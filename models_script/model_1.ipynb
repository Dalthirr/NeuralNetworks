{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary modules and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import keras\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#!pip install -q keras_metrics\n",
    "import keras_metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "#!pip install livelossplot\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytujemy poprzednio utworzone zbiory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../data/cleaned_data.csv', index_col=0)\n",
    "y = pd.read_csv('../data/labels.csv', header = None)\n",
    "\n",
    "X_test = pd.read_csv('../data/X_test.csv', index_col=0)\n",
    "X_train = pd.read_csv('../data/X_train.csv', index_col=0)\n",
    "X_train_std = pd.read_csv('../data/X_train_std.csv', index_col=0)\n",
    "X_test_std = pd.read_csv('../data/X_test_std.csv', index_col=0)\n",
    "y_train = pd.read_csv('../data/y_train.csv', index_col=0)\n",
    "y_test = pd.read_csv('../data/y_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zbuduję przykładową sieć, która składa się z warstwy wejściowej, jednej warstwy ukrytej i  warstwy wyjściowej.\n",
    "Kompilujemy model przy użyciu \n",
    "\n",
    "loss_function='binary_crossentropy';\n",
    "\n",
    "optimizer='Adam'.\n",
    "\n",
    "\n",
    "Naszą główną metryką będzie **precyzja** oznaczająca procent osób prawidłowie określonych jako zagrożone odejściem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pierwsze podejście do modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/3\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.4150 - precision: 0.6900 - acc: 0.8303 - val_loss: 3.4115 - val_precision: 0.3043 - val_acc: 0.7882\n",
      "Epoch 2/3\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3655 - precision: 0.7421 - acc: 0.8512 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 3/3\n",
      "6700/6700 [==============================] - 19s 3ms/step - loss: 0.3594 - precision: 0.7699 - acc: 0.8561 - val_loss: 7.0021 - val_precision: 0.1342 - val_acc: 0.5618\n",
      "3300/3300 [==============================] - 0s 30us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3837403065508062, 0.6790123455113549, 0.8403030303030303]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(11, activation='relu', input_shape=(10,)))\n",
    " \n",
    "model.add(Dense(121, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[keras_metrics.precision(),'accuracy'])\n",
    "                   \n",
    "model.fit(X_train_std, y_train,epochs=3, batch_size=1, verbose=1, validation_data=(X_test,y_test))\n",
    "\n",
    "model.evaluate(X_test_std,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprawdźmy skuteczność naszej predykcji raportem klasyfikacyjnym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90      2628\n",
      "           1       0.68      0.41      0.51       672\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      3300\n",
      "   macro avg       0.77      0.68      0.71      3300\n",
      "weighted avg       0.83      0.84      0.82      3300\n",
      "\n",
      "0.86286701208981\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_test_std)\n",
    "report = classification_report(y_test,y_pred)\n",
    "report1 = classification_report(y_test,y_pred,output_dict=True)\n",
    "print(report)\n",
    "print(report1['0']['precision'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdźmy, jak prezentują się wyniki w zależności od liczby neuronów w warstwie ukrytej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.4485 - precision: 0.5988 - acc: 0.8063 - val_loss: 6.9746 - val_precision: 0.1477 - val_acc: 0.5621\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3752 - precision: 0.7279 - acc: 0.8455 - val_loss: 7.3863 - val_precision: 0.1465 - val_acc: 0.5376\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3600 - precision: 0.7454 - acc: 0.8521 - val_loss: 7.5963 - val_precision: 0.1446 - val_acc: 0.5252\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3564 - precision: 0.7663 - acc: 0.8561 - val_loss: 10.2539 - val_precision: 0.1924 - val_acc: 0.3576\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3526 - precision: 0.7623 - acc: 0.8569 - val_loss: 10.9511 - val_precision: 0.1942 - val_acc: 0.3136\n",
      "Zakończono trenowanie modelu z 10 węzłami.\n",
      "3300/3300 [==============================] - 0s 35us/step\n",
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.4365 - precision: 0.6626 - acc: 0.8160 - val_loss: 3.6354 - val_precision: 0.2016 - val_acc: 0.7739\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.3690 - precision: 0.7477 - acc: 0.8513 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3565 - precision: 0.7690 - acc: 0.8564 - val_loss: 6.1481 - val_precision: 0.2223 - val_acc: 0.6155\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3502 - precision: 0.7624 - acc: 0.8563 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3451 - precision: 0.7729 - acc: 0.8612 - val_loss: 6.9771 - val_precision: 0.1347 - val_acc: 0.5630\n",
      "Zakończono trenowanie modelu z 30 węzłami.\n",
      "3300/3300 [==============================] - 0s 41us/step\n",
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.4169 - precision: 0.6616 - acc: 0.8251 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3666 - precision: 0.7379 - acc: 0.8504 - val_loss: 3.3016 - val_precision: 0.0000e+00 - val_acc: 0.7952\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3573 - precision: 0.7526 - acc: 0.8542 - val_loss: 3.2847 - val_precision: 0.0000e+00 - val_acc: 0.7958\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 23s 3ms/step - loss: 0.3501 - precision: 0.7703 - acc: 0.8578 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 23s 3ms/step - loss: 0.3472 - precision: 0.7719 - acc: 0.8603 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Zakończono trenowanie modelu z 50 węzłami.\n",
      "3300/3300 [==============================] - 0s 48us/step\n",
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 23s 3ms/step - loss: 0.4288 - precision: 0.6772 - acc: 0.8246 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.3674 - precision: 0.7643 - acc: 0.8542 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 23s 3ms/step - loss: 0.3577 - precision: 0.7581 - acc: 0.8533 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3529 - precision: 0.7798 - acc: 0.8593 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.3500 - precision: 0.7787 - acc: 0.8572 - val_loss: 4.2095 - val_precision: 0.2494 - val_acc: 0.7379\n",
      "Zakończono trenowanie modelu z 70 węzłami.\n",
      "3300/3300 [==============================] - 0s 41us/step\n",
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.4173 - precision: 0.6758 - acc: 0.8270 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.3663 - precision: 0.7320 - acc: 0.8493 - val_loss: 5.3204 - val_precision: 0.2376 - val_acc: 0.6679\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3567 - precision: 0.7672 - acc: 0.8555 - val_loss: 6.5901 - val_precision: 0.2325 - val_acc: 0.5879\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.3524 - precision: 0.7587 - acc: 0.8564 - val_loss: 12.4628 - val_precision: 0.2060 - val_acc: 0.2170\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 23s 3ms/step - loss: 0.3475 - precision: 0.7525 - acc: 0.8563 - val_loss: 11.4448 - val_precision: 0.1992 - val_acc: 0.2821\n",
      "Zakończono trenowanie modelu z 90 węzłami.\n",
      "3300/3300 [==============================] - 0s 41us/step\n",
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 23s 3ms/step - loss: 0.4245 - precision: 0.6488 - acc: 0.8187 - val_loss: 3.2725 - val_precision: 1.0000 - val_acc: 0.7970\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.3691 - precision: 0.7207 - acc: 0.8479 - val_loss: 3.4395 - val_precision: 0.1607 - val_acc: 0.7848\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 23s 3ms/step - loss: 0.3605 - precision: 0.7623 - acc: 0.8549 - val_loss: 4.4068 - val_precision: 0.2230 - val_acc: 0.7248\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.3530 - precision: 0.7734 - acc: 0.8590 - val_loss: 5.5357 - val_precision: 0.2336 - val_acc: 0.6539\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3498 - precision: 0.7858 - acc: 0.8600 - val_loss: 6.4123 - val_precision: 0.2317 - val_acc: 0.5991\n",
      "Zakończono trenowanie modelu z 110 węzłami.\n",
      "3300/3300 [==============================] - 0s 36us/step\n",
      "{50: [0.8614762386248737, 0.7837837837837838], 110: [0.8596017549780628, 0.7596439169139466], 70: [0.8619631901840491, 0.7295081967213115], 10: [0.8589394969408566, 0.7178770949720671], 90: [0.8609137055837564, 0.7565217391304347], 30: [0.865595075239398, 0.7420212765957447]}\n"
     ]
    }
   ],
   "source": [
    "nodes = [10, 30, 50, 70, 90, 110]\n",
    "wyniki = {}\n",
    "for i in nodes:\n",
    "    model = 0\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(11, activation='relu', input_shape=(10,)))\n",
    "\n",
    "    model.add(Dense(i, activation='relu'))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[keras_metrics.precision(), 'accuracy'])\n",
    "\n",
    "    model.fit(X_train_std, y_train, epochs=5, batch_size=1, verbose=1, validation_data=(X_test, y_test))\n",
    "    print(\"Zakończono trenowanie modelu z {0} węzłami.\".format(i))\n",
    "    model.evaluate(X_test_std, y_test)\n",
    "    y_pred = model.predict_classes(X_test_std)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    wyniki.update({i: [report['0']['precision'], report['1']['precision']]})\n",
    "\n",
    "print(wyniki)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wybieramy model z 90 węzłami - dał najlepsze wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/30\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.4562 - precision: 0.5911 - acc: 0.8024 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 2/30\n",
      "6700/6700 [==============================] - 23s 3ms/step - loss: 0.3984 - precision: 0.6775 - acc: 0.8337 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 3/30\n",
      "6700/6700 [==============================] - 23s 3ms/step - loss: 0.3760 - precision: 0.7425 - acc: 0.8469 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "3300/3300 [==============================] - 0s 38us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91      2628\n",
      "           1       0.81      0.34      0.48       672\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      3300\n",
      "   macro avg       0.83      0.66      0.69      3300\n",
      "weighted avg       0.84      0.85      0.82      3300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(10, activation='relu', input_shape=(10,)))\n",
    "model.add(Dropout(0.1, ))\n",
    "\n",
    "model.add(Dense(90, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[keras_metrics.precision(), 'accuracy'])\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "model.fit(X_train_std, y_train, epochs=30, batch_size=1, verbose=1, validation_data=(X_test, y_test),\n",
    "          callbacks=[early_stopping_monitor])\n",
    "\n",
    "model.evaluate(X_test_std, y_test)\n",
    "y_pred = model.predict_classes(X_test_std)\n",
    "report = classification_report(y_test, y_pred)\n",
    "report1 = classification_report(y_test, y_pred, output_dict=True)\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zapisujemy wytrenowany model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"../saved_models/model_1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autor: Tomasz Sołtysiak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
