{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary modules and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import keras\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#!pip install -q keras_metrics\n",
    "import keras_metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "#!pip install livelossplot\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytujemy poprzednio utworzone zbiory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../data/cleaned_data.csv', index_col=0)\n",
    "y = pd.read_csv('../data/labels.csv', header = None)\n",
    "\n",
    "X_test = pd.read_csv('../data/X_test.csv', index_col=0)\n",
    "X_train = pd.read_csv('../data/X_train.csv', index_col=0)\n",
    "X_train_std = pd.read_csv('../data/X_train_std.csv', index_col=0)\n",
    "X_test_std = pd.read_csv('../data/X_test_std.csv', index_col=0)\n",
    "y_train = pd.read_csv('../data/y_train.csv', index_col=0)\n",
    "y_test = pd.read_csv('../data/y_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zbuduję przykładową sieć, która składa się z warstwy wejściowej, jednej warstwy ukrytej i  warstwy wyjściowej.\n",
    "Kompilujemy model przy użyciu \n",
    "\n",
    "loss_function='binary_crossentropy';\n",
    "\n",
    "optimizer='Adam'.\n",
    "\n",
    "\n",
    "Naszą główną metryką będzie **precyzja** oznaczająca procent osób prawidłowie określonych jako zagrożone odejściem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pierwsze podejście do modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/3\n",
      "6700/6700 [==============================] - 19s 3ms/step - loss: 0.4307 - precision: 0.7170 - acc: 0.8237 - val_loss: 12.6759 - val_precision: 0.2039 - val_acc: 0.2048\n",
      "Epoch 2/3\n",
      "6700/6700 [==============================] - 19s 3ms/step - loss: 0.3651 - precision: 0.7437 - acc: 0.8513 - val_loss: 3.7012 - val_precision: 0.2442 - val_acc: 0.7697\n",
      "Epoch 3/3\n",
      "6700/6700 [==============================] - 19s 3ms/step - loss: 0.3529 - precision: 0.7472 - acc: 0.8546 - val_loss: 8.5212 - val_precision: 0.2396 - val_acc: 0.4658\n",
      "3300/3300 [==============================] - 0s 34us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3568253916321379, 0.7388888886836419, 0.8484848484848485]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(11, activation='relu', input_shape=(10,)))\n",
    " \n",
    "model.add(Dense(121, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[keras_metrics.precision(),'accuracy'])\n",
    "                   \n",
    "model.fit(X_train_std, y_train,epochs=3, batch_size=1, verbose=1, validation_data=(X_test,y_test))\n",
    "\n",
    "model.evaluate(X_test_std,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprawdźmy skuteczność naszej predykcji raportem klasyfikacyjnym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91      2628\n",
      "           1       0.74      0.40      0.52       672\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      3300\n",
      "   macro avg       0.80      0.68      0.71      3300\n",
      "weighted avg       0.84      0.85      0.83      3300\n",
      "\n",
      "0.861904761904762\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_test_std)\n",
    "report = classification_report(y_test,y_pred)\n",
    "report1 = classification_report(y_test,y_pred,output_dict=True)\n",
    "print(report)\n",
    "print(report1['0']['precision'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdźmy, jak prezentują się wyniki w zależności od liczby neuronów w warstwie ukrytej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 19s 3ms/step - loss: 0.4513 - precision: 0.6040 - acc: 0.8087 - val_loss: 7.9938 - val_precision: 0.1584 - val_acc: 0.4997\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3946 - precision: 0.7204 - acc: 0.8375 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 23s 3ms/step - loss: 0.3616 - precision: 0.7625 - acc: 0.8557 - val_loss: 3.2773 - val_precision: 1.0000 - val_acc: 0.7967\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.3533 - precision: 0.7621 - acc: 0.8578 - val_loss: 4.3597 - val_precision: 0.2075 - val_acc: 0.7255\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3496 - precision: 0.7734 - acc: 0.8600 - val_loss: 10.7266 - val_precision: 0.1863 - val_acc: 0.3279\n",
      "Zakończono trenowanie modelu z 10 węzłami.\n",
      "3300/3300 [==============================] - 0s 33us/step\n",
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.4390 - precision: 0.6318 - acc: 0.8151 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.3914 - precision: 0.7232 - acc: 0.8410 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3597 - precision: 0.7487 - acc: 0.8536 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 23s 3ms/step - loss: 0.3526 - precision: 0.7599 - acc: 0.8567 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 23s 3ms/step - loss: 0.3491 - precision: 0.7719 - acc: 0.8593 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Zakończono trenowanie modelu z 30 węzłami.\n",
      "3300/3300 [==============================] - 0s 32us/step\n",
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.4284 - precision: 0.6693 - acc: 0.8218 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.3686 - precision: 0.7586 - acc: 0.8525 - val_loss: 6.4548 - val_precision: 0.1378 - val_acc: 0.5909\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 23s 3ms/step - loss: 0.3573 - precision: 0.7510 - acc: 0.8546 - val_loss: 9.3247 - val_precision: 0.1749 - val_acc: 0.4145\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3524 - precision: 0.7506 - acc: 0.8551 - val_loss: 12.6210 - val_precision: 0.2044 - val_acc: 0.2082\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3474 - precision: 0.7687 - acc: 0.8594 - val_loss: 12.6718 - val_precision: 0.2039 - val_acc: 0.2052\n",
      "Zakończono trenowanie modelu z 50 węzłami.\n",
      "3300/3300 [==============================] - 0s 35us/step\n",
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.4407 - precision: 0.6322 - acc: 0.8119 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3974 - precision: 0.7052 - acc: 0.8360 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3592 - precision: 0.7645 - acc: 0.8576 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3522 - precision: 0.7735 - acc: 0.8597 - val_loss: 7.3467 - val_precision: 0.2358 - val_acc: 0.5397\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3471 - precision: 0.7920 - acc: 0.8616 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Zakończono trenowanie modelu z 70 węzłami.\n",
      "3300/3300 [==============================] - 0s 33us/step\n",
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.4203 - precision: 0.6679 - acc: 0.8245 - val_loss: 3.7432 - val_precision: 0.2844 - val_acc: 0.7670\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3659 - precision: 0.7213 - acc: 0.8499 - val_loss: 4.9788 - val_precision: 0.2416 - val_acc: 0.6894\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3605 - precision: 0.7523 - acc: 0.8537 - val_loss: 5.7467 - val_precision: 0.2342 - val_acc: 0.6409\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3573 - precision: 0.7620 - acc: 0.8564 - val_loss: 6.2827 - val_precision: 0.2322 - val_acc: 0.6070\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3528 - precision: 0.7577 - acc: 0.8566 - val_loss: 7.4928 - val_precision: 0.2338 - val_acc: 0.5306\n",
      "Zakończono trenowanie modelu z 90 węzłami.\n",
      "3300/3300 [==============================] - 0s 34us/step\n",
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/5\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.4321 - precision: 0.6737 - acc: 0.8184 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 2/5\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.3722 - precision: 0.7455 - acc: 0.8493 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 3/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3558 - precision: 0.7588 - acc: 0.8536 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 4/5\n",
      "6700/6700 [==============================] - 20s 3ms/step - loss: 0.3509 - precision: 0.7652 - acc: 0.8576 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 5/5\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3442 - precision: 0.7737 - acc: 0.8601 - val_loss: 7.5929 - val_precision: 0.2376 - val_acc: 0.5239\n",
      "Zakończono trenowanie modelu z 110 węzłami.\n",
      "3300/3300 [==============================] - 0s 36us/step\n",
      "{50: [0.8675450762829403, 0.6971153846153846], 110: [0.8596851471594799, 0.6931216931216931], 70: [0.861429534726905, 0.781437125748503], 10: [0.8762482168330956, 0.655241935483871], 90: [0.8632156324991429, 0.7127937336814621], 30: [0.8559919436052367, 0.7570093457943925]}\n"
     ]
    }
   ],
   "source": [
    "nodes = [10, 30, 50, 70, 90, 110]\n",
    "wyniki = {}\n",
    "for i in nodes:\n",
    "    model = 0\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(11, activation='relu', input_shape=(10,)))\n",
    "\n",
    "    model.add(Dense(i, activation='relu'))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[keras_metrics.precision(), 'accuracy'])\n",
    "\n",
    "    model.fit(X_train_std, y_train, epochs=5, batch_size=1, verbose=1, validation_data=(X_test, y_test))\n",
    "    print(\"Zakończono trenowanie modelu z {0} węzłami.\".format(i))\n",
    "    model.evaluate(X_test_std, y_test)\n",
    "    y_pred = model.predict_classes(X_test_std)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    wyniki.update({i: [report['0']['precision'], report['1']['precision']]})\n",
    "\n",
    "print(wyniki)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wybieramy model z 90 węzłami - dał najlepsze wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/30\n",
      "6700/6700 [==============================] - 22s 3ms/step - loss: 0.4308 - precision: 0.6687 - acc: 0.8204 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 2/30\n",
      "6700/6700 [==============================] - 23s 3ms/step - loss: 0.3834 - precision: 0.7049 - acc: 0.8410 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "Epoch 3/30\n",
      "6700/6700 [==============================] - 21s 3ms/step - loss: 0.3697 - precision: 0.7280 - acc: 0.8466 - val_loss: 3.2822 - val_precision: 0.0000e+00 - val_acc: 0.7964\n",
      "3300/3300 [==============================] - 0s 38us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91      2628\n",
      "           1       0.73      0.42      0.53       672\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      3300\n",
      "   macro avg       0.80      0.69      0.72      3300\n",
      "weighted avg       0.84      0.85      0.83      3300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(10, activation='relu', input_shape=(10,)))\n",
    "model.add(Dropout(0.1, ))\n",
    "\n",
    "model.add(Dense(90, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[keras_metrics.precision(), 'accuracy'])\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "model.fit(X_train_std, y_train, epochs=30, batch_size=1, verbose=1, validation_data=(X_test, y_test),\n",
    "          callbacks=[early_stopping_monitor])\n",
    "\n",
    "model.evaluate(X_test_std, y_test)\n",
    "y_pred = model.predict_classes(X_test_std)\n",
    "report = classification_report(y_test, y_pred)\n",
    "report1 = classification_report(y_test, y_pred, output_dict=True)\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zapisujemy wytrenowany model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../saved_models/model_1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autor: Tomasz Sołtysiak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
